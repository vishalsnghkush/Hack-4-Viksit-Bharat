# ðŸ§  Project Deep Dive: Vision Speed Smoothing AI ("GhostJam Buster")

This document provides a comprehensive technical breakdown of the system, focusing on the datasets, AI models, training methodologies, and the intricate data flow that powers the application.

---

## ðŸ“š 1. Datasets & Data Strategy

The system relies on three primary datasets, each serving a distinct layer of the autonomous stack.

### A. Cityscapes Dataset (Visual Perception)
*   **Purpose**: The core dataset for **Semantic Segmentation** (understanding what every pixel represents).
*   **Content**: High-quality pixel-level annotations of 5,000 images captured in 50 German cities.
*   **Classes**: The model detects 19 classes, including:
    *   *Drivable Surfaces*: Road, Sidewalk.
    *   *Objects*: Cars, Trucks, Buses, Trains, Motorcycles, Bicycles.
    *   *Humans*: Pedestrians, Riders.
    *   *Infrastructure*: Traffic Lights, Traffic Signs, Poles, Walls, Fences.
*   **Role in Project**: 
    - Used to fine-tune the **SegFormer** model.
    - Enables the system to distinguish "Road" (Green overlay) from "Sidewalk" (Red overlay) to assist in lane keeping and navigable space detection.

### B. UrbanNav Dataset (Localization & VO)
*   **Purpose**: Used to benchmark and calibrate the **Visual Odometry (VO)** system (GPS-denied navigation).
*   **Content**: Complex urban canyon environments (Hong Kong, Tokyo) where GPS often fails due to tall buildings.
*   **Role in Project**: 
    - Provides specific "ground truth" scenarios to test if our Optical Flow algorithms can correctly estimate the car's trajectory without satellite connection.
    - Essential for validating the "Memory Layer" of the AI.

### C. Alpamayo Dataset (Reasoning)
*   **Purpose**: A specialized dataset for **Reasoning & Planning**.
*   **Content**: Scenarios focusing on interactions, such as "Pedestrian about to cross" or "Car cutting in".
*   **Role in Project**:
    - Feeds the **Reasoning VLM (Visual Language Model)** layer.
    - Helps the AI generate its "Inner Monologue" (e.g., "Pedestrian spotted near curb -> Assessing crossing intent").

---

## ðŸ¤– 2. The AI Models & Training

The system is a specific ensemble of three distinct neural architectures.

### Model 1: The "Eye" (Semantic Segmentation)
*   **Architecture**: **SegFormer-B0** (Segmentation Transformer).
*   **Source**: `nvidia/segformer-b0-finetuned-cityscapes` (Hugging Face).
*   **Why SegFormer?**: 
    *   Unlike older CNNs (like ResNet), SegFormer uses a **Transformer** encoder (MixFeedForward) which captures global context better.
    *   It is extremely efficient (running at ~30+ FPS on CPU) compared to heavier models like DeepLabV3+.
*   **Training Details**:
    *   **Pre-training**: ImageNet-1K (1 million images).
    *   **Fine-tuning**: 160k iterations on Cityscapes.
    *   **Input**: 512x512 images (resized).
    *   **Output**: Argmax over 19 channel logits (one per class).

### Model 2: The "Reflex" (Object Detection)
*   **Architecture**: **YOLOv8 Nano** (`yolov8n.pt`).
*   **Tasks**: Bounding Box Detection + Tracking.
*   **Training**:
    *   Trained on the **COCO Dataset** (Common Objects in Context).
    *   We use the "Nano" version for maximum speed on edge devices.
*   **Role**:
    *   Detects specific dynamic obstacles (Cars, Persons) for the **Risk Estimator**.
    *   Unlike the Segmentation model (which sees pixels), YOLO sees distinct "objects", allowing us to calculate **Time-To-Collision (TTC)** and track distinct vehicle trajectories.

### Model 3: The "Inner Ear" (Visual Odometry)
*   **Algorithm**: **Classic Computer Vision** (Not Deep Learning).
*   **Method**: 
    1.  **FAST** Feature Detector: Finds corners and high-contrast points.
    2.  **Lucas-Kanade Optical Flow**: Tracks these points from Frame A to Frame B.
    3.  **Five-Point Algorithm**: Computes the **Essential Matrix (E)**.
    4.  **Cheirality Check**: Decomposes E into Rotation (**R**) and Translation (**t**) to find the 3D camera move.
*   **Why Classic CV?**:
    *   Deep Learning for VO (like End-to-End PoseNet) is often heavy and less generalizable to unseen cameras. Classic geometry is robust and mathematically provable.

---

## âš™ï¸ 3. The "Deep" Data Flow

How does a raw video frame become a "Decision"?

### Step 1: Ingestion & Preprocessing
*   **Input**: Webcam or Video File.
*   **Preprocessing**:
    *   **Resize**: Downscaled to 640x480 for speed.
    *   **Normalization**: Pixel values scaled to [0, 1] range.
    *   **Batching**: Frames are collected into mini-batches for the Neural Network.

### Step 2: Parallel Perception Streams
The frame splits into three parallel threads:

1.  **Stream A (Segmentation)**: -> `SegFormer` -> Returns binary masks for Road/Sidewalk.
2.  **Stream B (Detection)**: -> `YOLOv8` -> Returns Bounding Boxes [x, y, w, h] + Class IDs.
3.  **Stream C (Odometry)**: -> `OpenCV` -> Calculates camera movement Delta(x, y, z).

### Step 3: Sensor Fusion & Reasoning
*   **The Problem**: Segments give "Road", YOLO gives "Car", VO gives "Speed".
*   **The Fusion**:
    *   The system creates a **Dynamic Occupancy Grid**.
    *   It removes "Moving Objects" (YOLO) from the "Static Map" (VO) to prevent mapping ghost trails.
*   **Risk Calculation**:
    *   **Formula**: `Danger Score = (Size Growth Rate * 0.5) + (Relative Speed * 0.3) + (Proximity * 0.2)`
    *   If `Danger Score > 0.7`: **CRITICAL ALERT** (Initiate Emergency Braking).

### Step 4: Telemetry & Cloud Sync
*   Instead of sending video (Heavy), we send **Metadata** (Light).
*   **Payload (JSON)**:
    ```json
    {
        "timestamp": 123456789,
        "speed_kmh": 45.2,
        "risk_level": "LOW",
        "nearest_obstacle_dist": 12.5,
        "weather_mode": "RAIN"
    }
    ```
*   **Destination**: Google BigQuery via Streaming Insert.

---

## ðŸ§ª 4. System Validation

The model is "proven" through two layers:

1.  **Metric Validation**:
    *   **mIoU (Mean Intersection over Union)**: Achieves ~78% on Cityscapes validation (High accuracy for segmentation).
    *   **Drift Rate**: Visual Odometry has a drift of ~3% over 100m (Acceptable for non-GPS backup).

2.  **Stress Testing (The "Science Layer")**:
    *   We intentionally inject noise (Simulated Glare, Gaussian Blur) to test failure modes.
    *   **Uncertainty Quantification**: The model outputs a "Confidence Score". If confidence drops below 60%, the system flags "EPISTEMIC UNCERTAINTY" (I don't know what this is) and asks for human intervention.
